{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Image Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going learn [food images](https://www.kaggle.com/kmader/food41) from kaggle\n",
    "\n",
    "### Use the kaggle-cli to download the image\n",
    "\n",
    "```kaggle datasets download -d kmader/food41```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA  = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME = os.environ[\"HOME\"]\n",
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/.kaggle/datasets/kmader/food41/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA  = HOME+\"/.kaggle/datasets/kmader/food41/\"\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.3G\\t/home/paperspace/.kaggle/datasets/kmader/food41/food41.zip',\n",
       " '343M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n1000_r384x384x3.h5',\n",
       " '18M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r32x32x1.h5',\n",
       " '30M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r32x32x3.h5',\n",
       " '68M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r64x64x1.h5',\n",
       " '115M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_c101_n10099_r64x64x3.h5',\n",
       " '25M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r128x128x1.h5',\n",
       " '44M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r128x128x3.h5',\n",
       " '1.9M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r32x32x1.h5',\n",
       " '3.2M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r32x32x3.h5',\n",
       " '6.7M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r64x64x1.h5',\n",
       " '12M\\t/home/paperspace/.kaggle/datasets/kmader/food41/food_test_c101_n1000_r64x64x3.h5',\n",
       " '4.8G\\t/home/paperspace/.kaggle/datasets/kmader/food41/images.zip',\n",
       " '668K\\t/home/paperspace/.kaggle/datasets/kmader/food41/meta.zip']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!du -sh {DATA}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11240352\r\n",
      "-rw-rw-r-- 1 paperspace paperspace 5681757245 Aug  2 01:45 \u001b[0m\u001b[01;31mfood41.zip\u001b[0m\r\n",
      "-rw-rw-r-- 1 paperspace paperspace  359407496 Aug  2 01:45 food_c101_n1000_r384x384x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   18236331 Aug  2 01:45 food_c101_n10099_r32x32x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   30452874 Aug  2 01:45 food_c101_n10099_r32x32x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   70546750 Aug  2 01:45 food_c101_n10099_r64x64x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace  120306214 Aug  2 01:45 food_c101_n10099_r64x64x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   26042428 Aug  2 01:45 food_test_c101_n1000_r128x128x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   45897229 Aug  2 01:45 food_test_c101_n1000_r128x128x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace    1938115 Aug  2 01:45 food_test_c101_n1000_r32x32x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace    3294763 Aug  2 01:45 food_test_c101_n1000_r32x32x3.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace    6968764 Aug  2 01:45 food_test_c101_n1000_r64x64x1.h5\r\n",
      "-rw-rw-r-- 1 paperspace paperspace   11869967 Aug  2 01:45 food_test_c101_n1000_r64x64x3.h5\r\n",
      "drwxrwxr-x 2 paperspace paperspace       4096 Aug  2 11:08 \u001b[01;34mimages\u001b[0m/\r\n",
      "-rw-rw-r-- 1 paperspace paperspace 5132666035 Aug  2 01:45 \u001b[01;31mimages.zip\u001b[0m\r\n",
      "drwxrwxr-x 2 paperspace paperspace       4096 Aug  2 11:04 \u001b[01;34mmeta\u001b[0m/\r\n",
      "-rw-rw-r-- 1 paperspace paperspace     682599 Aug  2 01:45 \u001b[01;31mmeta.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p {DATA}images\n",
    "%mkdir -p {DATA}meta\n",
    "%ls -l {DATA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip and calculate the line of sub-folders: categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['102']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cd {DATA}/images; unzip ../images.zip > unzip.log;rm -rf unzip.log; ls -l|wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total 4140',\n",
       " '-rw-rw-r-- 1 paperspace paperspace    1184 Jul  9  2014 classes.txt',\n",
       " '-rw-rw-r-- 1 paperspace paperspace    1184 Sep 23  2013 labels.txt',\n",
       " '-rw-rw-r-- 1 paperspace paperspace  566868 Sep 23  2013 test.json',\n",
       " '-rw-rw-r-- 1 paperspace paperspace  489429 Sep 23  2013 test.txt',\n",
       " '-rw-rw-r-- 1 paperspace paperspace 1697751 Sep 21  2013 train.json',\n",
       " '-rw-rw-r-- 1 paperspace paperspace 1468812 Sep 21  2013 train.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!cd {DATA}/meta; unzip -q ../meta.zip; ls -l meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/.kaggle/datasets/kmader/food41/meta/meta/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "META = DATA + \"meta/meta/\"\n",
    "META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json preview ====================\n",
      "{\"churros\": [\"churros/1004234\", \"churros/1013460\",\n",
      "classes.txt preview ====================\n",
      "apple_pie\n",
      "baby_back_ribs\n",
      "baklava\n",
      "beef_carpaccio\n",
      "be\n",
      "train.txt preview ====================\n",
      "apple_pie/1005649\n",
      "apple_pie/1014775\n",
      "apple_pie/1026\n",
      "test.json preview ====================\n",
      "{\"churros\": [\"churros/1061830\", \"churros/1064042\",\n",
      "test.txt preview ====================\n",
      "apple_pie/1011328\n",
      "apple_pie/101251\n",
      "apple_pie/10343\n",
      "labels.txt preview ====================\n",
      "Apple pie\n",
      "Baby back ribs\n",
      "Baklava\n",
      "Beef carpaccio\n",
      "Be\n"
     ]
    }
   ],
   "source": [
    "for m_file in os.listdir(META):\n",
    "    print(m_file, \"preview\",\"=\"*20)\n",
    "    print(open(META+m_file).read()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/.kaggle/datasets/kmader/food41/images/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG = DATA+\"images/\"\n",
    "IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from ray.matchbox import Trainer\n",
    "from torchvision.models.densenet import densenet121 as feature_extractor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 256\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SCALE,SCALE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_set = ImageFolder(IMG,transform = transform, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train /Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set = ImageFolder(IMG,transform = transform, )\n",
    "val_set = ImageFolder(IMG,transform = transform, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pick = np.random.rand(len(img_set.samples))>0.8\n",
    "trn_pick = ~val_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set.samples = np.array(img_set.samples)[trn_pick].tolist()\n",
    "val_set.samples = np.array(img_set.samples)[val_pick].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80707, 20293)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_set),len(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 0.2796,  0.2796,  0.2796,  ...,  0.5878,  0.5707,  0.5364],\n",
       "           [ 0.2624,  0.1939,  0.1426,  ...,  0.5878,  0.4337,  0.4166],\n",
       "           [ 0.1597,  0.1939,  0.2282,  ...,  0.6221,  0.5707,  0.4679],\n",
       "           ...,\n",
       "           [ 0.5193,  0.5364,  0.4851,  ...,  0.6221,  0.5878,  0.5878],\n",
       "           [ 0.3823,  0.4851,  0.5022,  ...,  0.5878,  0.5707,  0.6049],\n",
       "           [ 0.4166,  0.5022,  0.5364,  ...,  0.5364,  0.5193,  0.5536]],\n",
       " \n",
       "          [[-0.0049, -0.0049, -0.0049,  ...,  0.4853,  0.5378,  0.5728],\n",
       "           [-0.0049, -0.0749, -0.1275,  ...,  0.4853,  0.3978,  0.4328],\n",
       "           [-0.0924, -0.0574, -0.0399,  ...,  0.5203,  0.5378,  0.4678],\n",
       "           ...,\n",
       "           [ 0.0651,  0.1001,  0.1001,  ...,  0.4153,  0.3978,  0.3978],\n",
       "           [-0.0574,  0.0651,  0.1176,  ...,  0.3803,  0.3627,  0.4153],\n",
       "           [-0.0224,  0.0651,  0.1527,  ...,  0.3277,  0.3102,  0.3452]],\n",
       " \n",
       "          [[-1.0724, -1.0376, -0.9853,  ...,  0.1825,  0.1651,  0.1651],\n",
       "           [-1.0550, -1.0898, -1.0724,  ...,  0.1999,  0.0431,  0.0431],\n",
       "           [-1.1073, -1.0376, -0.9678,  ...,  0.2348,  0.2173,  0.1128],\n",
       "           ...,\n",
       "           [-1.0027, -0.9330, -0.8981,  ..., -0.0615, -0.1138, -0.1138],\n",
       "           [-1.1770, -1.0201, -0.9156,  ..., -0.0441, -0.0790, -0.0615],\n",
       "           [-1.1596, -1.0201, -0.8981,  ..., -0.0790, -0.1138, -0.0964]]],\n",
       " \n",
       " \n",
       "         [[[-2.0837, -2.1008, -2.1008,  ..., -1.9809, -1.9809, -1.9980],\n",
       "           [-2.0837, -2.1008, -2.1008,  ..., -1.9638, -1.9809, -1.9980],\n",
       "           [-2.1008, -2.1008, -2.1179,  ..., -1.9638, -1.9809, -1.9980],\n",
       "           ...,\n",
       "           [-0.0629, -0.0287, -0.0972,  ..., -1.4843, -1.5357, -1.5699],\n",
       "           [-0.0972, -0.0287, -0.0458,  ..., -1.5357, -1.5699, -1.6042],\n",
       "           [-0.1486, -0.0801, -0.0287,  ..., -1.5528, -1.5870, -1.6213]],\n",
       " \n",
       "          [[-2.0182, -2.0357, -2.0357,  ..., -1.8957, -1.8957, -1.9132],\n",
       "           [-2.0182, -2.0357, -2.0357,  ..., -1.9132, -1.9307, -1.9482],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -1.9132, -1.9307, -1.9482],\n",
       "           ...,\n",
       "           [-0.6702, -0.6527, -0.7577,  ..., -1.7031, -1.7381, -1.7731],\n",
       "           [-0.7052, -0.6877, -0.7052,  ..., -1.7206, -1.7556, -1.7731],\n",
       "           [-0.7577, -0.7227, -0.7227,  ..., -1.7031, -1.7381, -1.7731]],\n",
       " \n",
       "          [[-1.6824, -1.6999, -1.6999,  ..., -1.6302, -1.6302, -1.6476],\n",
       "           [-1.6824, -1.6999, -1.6999,  ..., -1.6302, -1.6476, -1.6650],\n",
       "           [-1.6999, -1.6999, -1.7173,  ..., -1.6302, -1.6476, -1.6650],\n",
       "           ...,\n",
       "           [-0.8807, -0.8981, -0.9853,  ..., -1.4733, -1.5081, -1.5430],\n",
       "           [-0.8633, -0.8807, -0.9678,  ..., -1.4907, -1.5256, -1.5604],\n",
       "           [-0.8807, -0.8981, -0.9678,  ..., -1.4907, -1.5256, -1.5604]]]]),\n",
       " ('89', '85')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(DataLoader(trn_set,batch_size=2,shuffle=True))\n",
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(in_, out_, ks=3, activation = nn.LeakyReLU()):\n",
    "    return nn.Sequential(*[\n",
    "        nn.Conv2d(in_, out_, kernel_size=ks, padding=ks//3, bias = False),\n",
    "        nn.BatchNorm2d(out_,),\n",
    "        activation,\n",
    "    ])\n",
    "\n",
    "def conv_block(in_,out_,nb_layers):\n",
    "    layers = []\n",
    "    for i in range(nb_layers):\n",
    "        if i == 0: layers.append(conv_layer(in_,out_))\n",
    "        elif i == nb_layers - 1:layers.append(conv_layer(out_,out_,activation = nn.MaxPool2d((2,2))))\n",
    "        else: layers.append(conv_layer(out_,out_))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        a pytorch version of Flatten layer\n",
    "        \"\"\"\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(x):\n",
    "    \"\"\"\n",
    "    Arg max of a torch tensor (2 dimensional, dim=1)\n",
    "    :param x:  torch tensor\n",
    "    :return: index the of the max\n",
    "    \"\"\"\n",
    "    return torch.max(x, dim=1)[1]\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_pred: predition of y (will be argmaxed)\n",
    "    :param y_true: true label of y (index)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return (argmax(y_pred) == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,path):\n",
    "    \"\"\"\n",
    "    model:pytorch model\n",
    "    path:save to path, end with pkl\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(model,path):\n",
    "    model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "conv_model = feature_extractor(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = conv_model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dense_conv1 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"conv0\",\"norm0\",\"relu0\",\"pool0\",\"denseblock1\",\"transition1\",\n",
    "#                                                                                    \"denseblock2\",\"transition2\",\"denseblock3\",\"transition3\",]])\n",
    "\n",
    "# dense_conv2 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"denseblock4\",\"norm5\"]])\n",
    "\n",
    "# conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_WIDTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  top_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(top_half,self).__init__()\n",
    "        self.top_ = nn.Sequential(*[nn.AdaptiveAvgPool2d((1,1)),Flatten(),\n",
    "                                   nn.Linear(FEATURE_WIDTH,len(img_set.classes),bias=True),\n",
    "                                   ])\n",
    "    def forward(self,x):\n",
    "        return F.softmax(self.top_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half_  = top_half()\n",
    "\n",
    "if CUDA:\n",
    "    top_half_.cuda()\n",
    "    conv_layers.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(*args,**kwargs):\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    opt.zero_grad()\n",
    "    y_ = top_half_(conv_layers(x)))\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}\n",
    "\n",
    "def val_action(*args,**kwargs):\n",
    "    x,y = args[0]\n",
    "    y = torch.LongTensor(np.array(y).astype(int))\n",
    "    \n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    y_ = top_half_(dense_conv2(dense_conv1(x)))\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    acc = accuracy(y_,y)\n",
    "    \n",
    "    return {\"loss\":loss.item(),\n",
    "            \"acc\":acc.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = Adam(list(dense_conv2.parameters())+list(top_half_.parameters()))\n",
    "\n",
    "trainer = Trainer(trn_set, val_dataset = val_set, batch_size = 32, print_on = 5)\n",
    "\n",
    "trainer.action = action\n",
    "trainer.val_action = val_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(dense_conv2,\"food_dense_conv2.0.0.1.npy\")\n",
    "# load_model(top_half_,\"food_top.0.0.1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2531 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-40839d992a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/ray/matchbox.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, name, log_addr)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mkdir -p %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/ray/matchbox.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"iter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2b8b78118b72>\u001b[0m in \u001b[0;36maction\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_half_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_conv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_conv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(dense_conv2,\"food_dense_conv2.0.0.1.npy\")\n",
    "save_model(top_half_,\"food_top.0.0.1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
